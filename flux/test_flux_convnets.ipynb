{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifies MNIST digits with a convolutional network.\n",
    "# Writes out saved model to the file \"mnist_conv.bson\".\n",
    "# Demonstrates basic model construction, training, saving,\n",
    "# conditional early-exit, and learning rate scheduling.\n",
    "#\n",
    "# This model, while simple, should hit around 99% test\n",
    "# accuracy after training for approximately 20 epochs.\n",
    "\n",
    "using Flux, Flux.Data.MNIST, Statistics\n",
    "using Flux: onehotbatch, onecold, logitcrossentropy\n",
    "using Base.Iterators: partition\n",
    "using Printf, BSON\n",
    "using Parameters: @with_kw\n",
    "using CUDAapi\n",
    "if has_cuda()\n",
    "    @info \"CUDA is on\"\n",
    "    import CuArrays\n",
    "    CuArrays.allowscalar(false)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Args"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@with_kw mutable struct Args\n",
    "    lr::Float64 = 3e-3\n",
    "    epochs::Int = 20\n",
    "    batch_size = 128\n",
    "    savepath::String = \"./\" \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make_minibatch (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bundle images together with labels and group into minibatchess\n",
    "function make_minibatch(X, Y, idxs)\n",
    "    X_batch = Array{Float32}(undef, size(X[1])..., 1, length(idxs))\n",
    "    for i in 1:length(idxs)\n",
    "        X_batch[:, :, :, i] = Float32.(X[idxs[i]])\n",
    "    end\n",
    "    Y_batch = onehotbatch(Y[idxs], 0:9)\n",
    "    return (X_batch, Y_batch)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_processed_data (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_processed_data(args)\n",
    "    # Load labels and images from Flux.Data.MNIST\n",
    "    train_labels = MNIST.labels()\n",
    "    train_imgs = MNIST.images()\n",
    "    mb_idxs = partition(1:length(train_imgs), args.batch_size)\n",
    "    train_set = [make_minibatch(train_imgs, train_labels, i) for i in mb_idxs] \n",
    "    \n",
    "    # Prepare test set as one giant minibatch:\n",
    "    test_imgs = MNIST.images(:test)\n",
    "    test_labels = MNIST.labels(:test)\n",
    "    test_set = make_minibatch(test_imgs, test_labels, 1:length(test_imgs))\n",
    "\n",
    "    return train_set, test_set\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "build_model (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build model\n",
    "function build_model(args; imgsize = (28,28,1), nclasses = 10)\n",
    "    cnn_output_size = Int.(floor.([imgsize[1]/8,imgsize[2]/8,32]))\n",
    "\n",
    "    return Chain(\n",
    "    # First convolution, operating upon a 28x28 image\n",
    "    Conv((3, 3), imgsize[3]=>16, pad=(1,1), relu),\n",
    "    MaxPool((2,2)),\n",
    "\n",
    "    # Second convolution, operating upon a 14x14 image\n",
    "    Conv((3, 3), 16=>32, pad=(1,1), relu),\n",
    "    MaxPool((2,2)),\n",
    "\n",
    "    # Third convolution, operating upon a 7x7 image\n",
    "    Conv((3, 3), 32=>32, pad=(1,1), relu),\n",
    "    MaxPool((2,2)),\n",
    "\n",
    "    # Reshape 3d tensor into a 2d one using `Flux.flatten`, at this point it should be (3, 3, 32, N)\n",
    "    flatten,\n",
    "    Dense(prod(cnn_output_size), 10))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We augment `x` a little bit here, adding in random noise. \n",
    "augment(x) = x .+ gpu(0.1f0*randn(eltype(x), size(x)))\n",
    "\n",
    "# Returns a vector of all parameters used in model\n",
    "paramvec(m) = vcat(map(p->reshape(p, :), params(m))...)\n",
    "\n",
    "# Function to check if any element is NaN or not\n",
    "anynan(x) = any(isnan.(x))\n",
    "\n",
    "accuracy(x, y, model) = mean(onecold(cpu(model(x))) .== onecold(cpu(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train(; kws...)\n",
    "    args = Args(; kws...)\n",
    "\n",
    "    @info(\"Loading data set\")\n",
    "    train_set, test_set = get_processed_data(args)\n",
    "\n",
    "    # Define our model.  We will use a simple convolutional architecture with\n",
    "    # three iterations of Conv -> ReLU -> MaxPool, followed by a final Dense layer.\n",
    "    @info(\"Building model...\")\n",
    "    model = build_model(args) \n",
    "\n",
    "    # Load model and datasets onto GPU, if enabled\n",
    "    train_set = gpu.(train_set)\n",
    "    test_set = gpu.(test_set)\n",
    "    model = gpu(model)\n",
    "    \n",
    "    # Make sure our model is nicely precompiled before starting our training loop\n",
    "    model(train_set[1][1])\n",
    "\n",
    "    # `loss()` calculates the crossentropy loss between our prediction `y_hat`\n",
    "    # (calculated from `model(x)`) and the ground truth `y`.  We augment the data\n",
    "    # a bit, adding gaussian random noise to our image to make it more robust.\n",
    "    function loss(x, y)\n",
    "        x̂ = augment(x)\n",
    "        ŷ = model(x̂)\n",
    "        return logitcrossentropy(ŷ, y)\n",
    "    end\n",
    "\n",
    "    # Train our model with the given training set using the ADAM optimizer and\n",
    "    # printing out performance against the test set as we go.\n",
    "    opt = ADAM(args.lr)\n",
    "\n",
    "    @info(\"Beginning training loop...\")\n",
    "    best_acc = 0.0\n",
    "    last_improvement = 0\n",
    "    for epoch_idx in 1:args.epochs\n",
    "        # Train for a single epoch\n",
    "        Flux.train!(loss, params(model), train_set, opt)\n",
    "    \n",
    "        # Terminate on NaN\n",
    "        if anynan(paramvec(model))\n",
    "            @error \"NaN params\"\n",
    "            break\n",
    "        end\n",
    "\n",
    "        # Calculate accuracy:\n",
    "        acc = accuracy(test_set..., model)\n",
    "\n",
    "        @info(@sprintf(\"[%d]: Test accuracy: %.4f\", epoch_idx, acc))\n",
    "        # If our accuracy is good enough, quit out.\n",
    "        if acc >= 0.999\n",
    "            @info(\" -> Early-exiting: We reached our target accuracy of 99.9%\")\n",
    "            break\n",
    "        end\n",
    "\n",
    "        # If this is the best accuracy we've seen so far, save the model out\n",
    "        if acc >= best_acc\n",
    "            @info(\" -> New best accuracy! Saving model out to mnist_conv.bson\")\n",
    "            BSON.@save joinpath(args.savepath, \"mnist_conv.bson\") params=cpu.(params(model)) epoch_idx acc\n",
    "            best_acc = acc\n",
    "            last_improvement = epoch_idx\n",
    "        end\n",
    "\n",
    "        # If we haven't seen improvement in 5 epochs, drop our learning rate:\n",
    "        if epoch_idx - last_improvement >= 5 && opt.eta > 1e-6\n",
    "            opt.eta /= 10.0\n",
    "            @warn(\" -> Haven't improved in a while, dropping learning rate to $(opt.eta)!\")\n",
    "   \n",
    "            # After dropping learning rate, give it a few epochs to improve\n",
    "            last_improvement = epoch_idx\n",
    "        end\n",
    "\n",
    "        if epoch_idx - last_improvement >= 10\n",
    "            @warn(\" -> We're calling this converged.\")\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test (generic function with 1 method)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the model, from saved model\n",
    "function test(; kws...)\n",
    "    args = Args(; kws...)\n",
    "    \n",
    "    # Loading the test data\n",
    "    _,test_set = get_processed_data(args)\n",
    "    \n",
    "    # Re-constructing the model with random initial weights\n",
    "    model = build_model(args)\n",
    "    \n",
    "    # Loading the saved parameters\n",
    "    BSON.@load joinpath(args.savepath, \"mnist_conv.bson\") params\n",
    "    \n",
    "    # Loading parameters onto the model\n",
    "    Flux.loadparams!(model, params)\n",
    "    \n",
    "    test_set = gpu.(test_set)\n",
    "    model = gpu(model)\n",
    "    @show accuracy(test_set...,model)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Loading data set\n",
      "└ @ Main In[17]:4\n",
      "┌ Info: Building model...\n",
      "└ @ Main In[17]:9\n",
      "┌ Info: Beginning training loop...\n",
      "└ @ Main In[17]:33\n",
      "┌ Info: [1]: Test accuracy: 0.9815\n",
      "└ @ Main In[17]:49\n",
      "┌ Info:  -> New best accuracy! Saving model out to mnist_conv.bson\n",
      "└ @ Main In[17]:58\n",
      "┌ Info: [2]: Test accuracy: 0.9876\n",
      "└ @ Main In[17]:49\n",
      "┌ Info:  -> New best accuracy! Saving model out to mnist_conv.bson\n",
      "└ @ Main In[17]:58\n",
      "┌ Info: [3]: Test accuracy: 0.9897\n",
      "└ @ Main In[17]:49\n",
      "┌ Info:  -> New best accuracy! Saving model out to mnist_conv.bson\n",
      "└ @ Main In[17]:58\n",
      "┌ Info: [4]: Test accuracy: 0.9900\n",
      "└ @ Main In[17]:49\n",
      "┌ Info:  -> New best accuracy! Saving model out to mnist_conv.bson\n",
      "└ @ Main In[17]:58\n",
      "┌ Info: [5]: Test accuracy: 0.9913\n",
      "└ @ Main In[17]:49\n",
      "┌ Info:  -> New best accuracy! Saving model out to mnist_conv.bson\n",
      "└ @ Main In[17]:58\n",
      "┌ Info: [6]: Test accuracy: 0.9903\n",
      "└ @ Main In[17]:49\n",
      "┌ Info: [7]: Test accuracy: 0.9913\n",
      "└ @ Main In[17]:49\n",
      "┌ Info:  -> New best accuracy! Saving model out to mnist_conv.bson\n",
      "└ @ Main In[17]:58\n",
      "┌ Info: [8]: Test accuracy: 0.9907\n",
      "└ @ Main In[17]:49\n",
      "┌ Info: [9]: Test accuracy: 0.9902\n",
      "└ @ Main In[17]:49\n",
      "┌ Info: [10]: Test accuracy: 0.9885\n",
      "└ @ Main In[17]:49\n",
      "┌ Info: [11]: Test accuracy: 0.9873\n",
      "└ @ Main In[17]:49\n",
      "┌ Info: [12]: Test accuracy: 0.9906\n",
      "└ @ Main In[17]:49\n",
      "┌ Warning:  -> Haven't improved in a while, dropping learning rate to 0.00030000000000000003!\n",
      "└ @ Main In[17]:67\n",
      "┌ Info: [13]: Test accuracy: 0.9922\n",
      "└ @ Main In[17]:49\n",
      "┌ Info:  -> New best accuracy! Saving model out to mnist_conv.bson\n",
      "└ @ Main In[17]:58\n",
      "┌ Info: [14]: Test accuracy: 0.9929\n",
      "└ @ Main In[17]:49\n",
      "┌ Info:  -> New best accuracy! Saving model out to mnist_conv.bson\n",
      "└ @ Main In[17]:58\n",
      "┌ Info: [15]: Test accuracy: 0.9931\n",
      "└ @ Main In[17]:49\n",
      "┌ Info:  -> New best accuracy! Saving model out to mnist_conv.bson\n",
      "└ @ Main In[17]:58\n",
      "┌ Info: [16]: Test accuracy: 0.9929\n",
      "└ @ Main In[17]:49\n",
      "┌ Info: [17]: Test accuracy: 0.9926\n",
      "└ @ Main In[17]:49\n",
      "┌ Info: [18]: Test accuracy: 0.9924\n",
      "└ @ Main In[17]:49\n",
      "┌ Info: [19]: Test accuracy: 0.9925\n",
      "└ @ Main In[17]:49\n",
      "┌ Info: [20]: Test accuracy: 0.9921\n",
      "└ @ Main In[17]:49\n",
      "┌ Warning:  -> Haven't improved in a while, dropping learning rate to 3.0000000000000004e-5!\n",
      "└ @ Main In[17]:67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy(test_set..., model) = 0.9931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9931"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cd(@__DIR__) \n",
    "train()\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
